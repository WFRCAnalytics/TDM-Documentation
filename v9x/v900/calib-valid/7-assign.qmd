---
margin-header: Version 9.0.0 - Calibration/Validation
echo: false
warning: false
message: false
---

# Highway Assignment

## Table Summaries

```{python}
import pandas as pd
import numpy as np
allveh_valid = pd.read_csv("data/assign/wf-validation-04-14-2023-AllVeh.csv")
value_vars = ['Valid_FC', 'Valid_All']
allveh_melted = allveh_valid.melt(id_vars=['CO_FIPS', 'SEGID', 'ATYPENAME', 'Mod_AWDT', 'Obs_AWDT', 'Diff', 'Pct_Dev', 'Diff_Sq',
                                   'Mod_Car', 'Obs_Car', 'Diff_Car', 'Pct_Dev_Car', 'Diff_Sq_Car', 'Mod_MD', 'Obs_MD',
                                   'Diff_MD', 'Pct_Dev_MD', 'Diff_Sq_MD', 'Mod_HV', 'Obs_HV', 'Diff_HV', 'Pct_Dev_HV',
                                   'Diff_Sq_HV', 'DY_VMT','DY_VMT.1'], value_vars=value_vars, var_name='Valid_Type', value_name='Valid')
allveh_melted = allveh_melted[['CO_FIPS', 'SEGID', 'Valid', 'ATYPENAME', 'Mod_AWDT', 'Obs_AWDT', 'Diff', 'Pct_Dev', 'Diff_Sq',
                 'Mod_Car', 'Obs_Car', 'Diff_Car', 'Pct_Dev_Car', 'Diff_Sq_Car', 'Mod_MD', 'Obs_MD', 'Diff_MD',
                 'Pct_Dev_MD', 'Diff_Sq_MD', 'Mod_HV', 'Obs_HV', 'Diff_HV', 'Pct_Dev_HV', 'Diff_Sq_HV', 'DY_VMT','DY_VMT.1']]
allveh_melted = allveh_melted.rename(columns = {'DY_VMT':'ModelVMT', 'DY_VMT.1':'ObserveVMT'})
allveh_melted
```

```{python}
allveh_melted2 = allveh_melted.copy()
allveh_melted2['CO_FIPS'] = 'All'
allveh = pd.concat([allveh_melted, allveh_melted2])
```

```{python}
agg_functions = {'SEGID'   : 'count',
                 'Mod_AWDT': 'mean',
                 'Obs_AWDT': 'mean',
                 'Diff_Sq' : lambda x: np.sqrt(np.sum(x) / (x.nunique() - 1)),# this is right, excel is wrong (excel is only using sum at county level)
                 'ModelVMT': 'sum',
                 'ObserveVMT': 'sum'} 

#summarize allveh_melted to create daily comparison by facility type tables
allveh_sum = allveh.groupby(['CO_FIPS','Valid']).agg(agg_functions).reset_index()
allveh_sum = allveh_sum.rename(columns={'SEGID':'NumberOfSegs', 'Mod_AWDT':'ModelVolume', 'Obs_AWDT':'CountVolume', 'Valid':'FunctionalType'})

allveh_sum['VolDiff'] = allveh_sum['ModelVolume'] - allveh_sum['CountVolume']
allveh_sum['VolPctDiff'] = (allveh_sum['ModelVolume'] / allveh_sum['CountVolume'] - 1) * 100
allveh_sum['RMSE'] = allveh_sum['Diff_Sq']
allveh_sum = allveh_sum.drop(columns={'Diff_Sq'})
allveh_sum['PctRMSE'] = (allveh_sum['RMSE'] / allveh_sum['CountVolume']) * 100
allveh_sum['VMTDiff'] = allveh_sum['ModelVMT'] - allveh_sum['ObserveVMT']
allveh_sum['VMTPctDiff'] = (allveh_sum['VMTDiff'] / allveh_sum['ObserveVMT']) * 100
allveh_sum_copy = allveh_sum.copy()

number_columns = allveh_sum.select_dtypes(include=['float64', 'int64']).columns
allveh_sum[number_columns] = allveh_sum[number_columns].round().astype(int)
allveh_sum['VolPctDiff'] = allveh_sum['VolPctDiff'].astype(str) + '%'
allveh_sum['PctRMSE'] = allveh_sum['PctRMSE'].astype(str) + '%'
allveh_sum['VMTPctDiff'] = allveh_sum['VMTPctDiff'].astype(str) + '%'
```

```{python}
allveh_vol = allveh_sum[['CO_FIPS','FunctionalType','NumberOfSegs','ModelVolume','CountVolume', 'VolDiff', 'VolPctDiff', 'RMSE', 'PctRMSE']]
allveh_vmt = allveh_sum[['CO_FIPS','FunctionalType','NumberOfSegs','ModelVMT','ObserveVMT', 'VMTDiff', 'VMTPctDiff']]
```


### Regional Summaries 
```{python}
allveh_pct = allveh_sum[['CO_FIPS','FunctionalType','VolPctDiff','VMTPctDiff']]
allveh_pct = allveh_pct.rename(columns={'VolPctDiff':'Volume', 'VMTPctDiff':'VMT'})
allveh_pct = allveh_pct.melt(id_vars = ['CO_FIPS','FunctionalType'], value_vars = ['Volume', 'VMT'], var_name = 'Variable',value_name = 'Value')
allveh_pct = allveh_pct.pivot(index=['FunctionalType','Variable'], columns='CO_FIPS', values='Value').reset_index()
allveh_pct = allveh_pct.rename(columns={3:'Box Elder', 11: 'Davis', 35: 'Salt Lake', 49: 'Utah', 57: 'Weber'})
allveh_pct = allveh_pct.sort_values(by = ['Variable','FunctionalType'], ascending =[False,False])
allveh_pct = allveh_pct[['Variable', 'FunctionalType','All', 'Box Elder', 'Weber', 'Davis', 'Salt Lake', 'Utah']]
```

```{python}
allveh_pct_exact = allveh_sum_copy[['CO_FIPS','FunctionalType','VolPctDiff','VMTPctDiff']]
allveh_pct_exact = allveh_pct_exact.rename(columns={'VolPctDiff':'Volume', 'VMTPctDiff':'VMT'})
allveh_pct_exact = allveh_pct_exact.melt(id_vars = ['CO_FIPS','FunctionalType'], value_vars = ['Volume', 'VMT'], var_name = 'Variable',value_name = 'Value')
allveh_pct_exact = allveh_pct_exact.pivot(index=['FunctionalType','Variable'], columns='CO_FIPS', values='Value').reset_index()
allveh_pct_exact = allveh_pct_exact.rename(columns={3:'Box Elder', 11: 'Davis', 35: 'Salt Lake', 49: 'Utah', 57: 'Weber'})
allveh_pct_exact = allveh_pct_exact.sort_values(by = ['Variable','FunctionalType'], ascending =[False,False])
allveh_pct_exact = allveh_pct_exact[['Variable', 'FunctionalType','All', 'Box Elder', 'Weber', 'Davis', 'Salt Lake', 'Utah']]
allveh_pct_exact = allveh_pct_exact.melt(id_vars = ['FunctionalType', 'Variable'], value_vars = ['All', 'Box Elder', 'Weber', 'Davis', 'Salt Lake', 'Utah'], value_name = 'Value', var_name = 'Region')
```

```{python}
allveh_abs_exact = allveh_sum_copy[['CO_FIPS', 'FunctionalType', 'ModelVolume', 'CountVolume', 'ModelVMT', 'ObserveVMT']]
allveh_abs_exact = allveh_abs_exact.melt(id_vars = ['CO_FIPS','FunctionalType'], value_vars = ['ModelVolume', 'CountVolume', 'ModelVMT', 'ObserveVMT'], var_name = 'DataSource',value_name = 'Value')
allveh_abs_exact['Variable'] = allveh_abs_exact['DataSource'].apply(lambda x: 'Volume' if 'Volume' in x else 'VMT')
allveh_abs_exact['DataSource'] = allveh_abs_exact['DataSource'].apply(lambda x: 'Model' if 'Model' in x else 'Observed')
allveh_abs_region = allveh_abs_exact[allveh_abs_exact['CO_FIPS'] == 'All']
```

```{python}
ojs_define(vvpct = allveh_pct)
ojs_define(vvpctLong = allveh_pct_exact)
ojs_define(vvabsLong = allveh_abs_exact)
ojs_define(vvabsLongR = allveh_abs_region)
```

```{ojs}
viewof vvSelect = Inputs.select(new Map([['Volume','Volume'], ['VMT','VMT']]), {value: 'Variable', label: "Select Summary Variable:"})
//viewof bCountySelect2 = Inputs.select(new Map([['Box Elder',3], ['Weber',57], ['Davis',11], ['Salt Lake',35], ['Utah',49], ['Region', 'All']]), {value: 'CO_FIPS', label: "Select Region:"})
```

```{ojs}
vvp = transpose(vvpct)
vvpL = transpose(vvpctLong)
vvaL = transpose(vvabsLong)
vvaLR = transpose(vvabsLongR)
table_vvData = vvp.filter(function(dataL) {
    return vvSelect == dataL.Variable;
})
stack_vvaData = vvaLR.filter(function(dataL) {
    return vvSelect == dataL.Variable; //&&
           //bCountySelect2 == dataL.CO_FIPS;
})
```

::: {.panel-tabset}
### Model v. Observed
```{ojs}
import {GroupedBarChart} from "@d3/grouped-bar-chart"
import {Legend, Swatches} from "@d3/color-legend"
import {howto, altplot} from "@d3/example-components"
chart2 = GroupedBarChart(stack_vvaData, {
    x: d => d.FunctionalType,
    y: d => d.Value,
    z: d => d.DataSource,
    yLabel: "Value",
    zDomain: ['Model','Observed'],
    width,
    height: 500,
    colors: ["#376092", "#77933c"]
})
```

### Regional Summary Table
```{ojs}
//| echo: false
Inputs.table(table_vvData, {
  style: {
    fontSize: 16,
  },
  columns: [
    "FunctionalType",
    "All",
    "Box Elder",
    "Weber",
    'Davis',
    'Salt Lake',
    'Utah'
  ],
  header: {
    FunctionalType:"FT",
    All: 'Region'
  }})
```
:::

### Average Daily Comparison
```{python}
ojs_define(vol = allveh_vol)
ojs_define(vmt = allveh_vmt)
```

```{ojs}
viewof bCountySelect = Inputs.select(new Map([['Box Elder',3], ['Weber',57], ['Davis',11], ['Salt Lake',35], ['Utah',49], ['Region', 'All']]), {value: 'CO_FIPS', label: "Select Region:"})
```

```{ojs}
volT = transpose(vol)
vmtT = transpose(vmt)
filtered_volData = volT.filter(function(dataL) {
    return bCountySelect == dataL.CO_FIPS;
})
filtered_vmtData = vmtT.filter(function(dataL){
    return bCountySelect == dataL.CO_FIPS;
})
```

**Table 1: Average Daily Comparison by Facility Type (Volume).**
```{ojs}
//| echo: false
Inputs.table(filtered_volData, {
  style: {
    fontSize: 16,
  },
  columns: [
    "FunctionalType",
    "NumberOfSegs",
    "ModelVolume",
    "CountVolume",
    'VolDiff',
    'VolPctDiff',
    'RMSE',
    'PctRMSE'
  ],
  header: {
    FunctionalType:"FT",
    NumberOfSegs:"NumSegs",
    ModelVolume:"Model",
    CountVolume:"Observe",
    VolDiff:'Diff',
    VolPctDiff:'PctDiff',
    RMSE:'RMSE',
    PctRMS:'PctRMSE'
  }})

```

**Table 2: Average Daily Comparison by Facility Type (VMT).**
```{ojs}
//| echo: false
Inputs.table(filtered_vmtData, {
  style: {
    fontSize: 16,
  },
  columns: [
    "FunctionalType",
    "NumberOfSegs",
    "ModelVMT",
    "ObserveVMT",
    'VMTDiff',
    'VMTPctDiff'
  ],
  header: {
    FunctionalType:"FT",
    NumberOfSegs:"NumSegs",
    ModelVMT:"Model",
    ObserveVMT:"Observe",
    VMTDiff:'Diff',
    VMTPctDiff:'PctDiff'
  }})
```

## Validation Charts

<mark>???Hard to find regression model that starts at (0,0) -- Good enough???</mark>

<mark>???Hard to create pct error graphs with bins???</mark>

<mark>???Should I just do screen shots of the validation charts???</mark>

```{python}
allveh_validC = allveh_valid[['Mod_AWDT','Obs_AWDT','Pct_Dev', 'Mod_Car','Obs_Car','Pct_Dev_Car','Mod_MD', 'Obs_MD', 'Pct_Dev_MD', 'Mod_HV', 'Obs_HV', 'Pct_Dev_HV']]
allveh_validC['row_number'] = range(1, len(allveh_validC) + 1)
allveh_validC = pd.melt(
    allveh_validC,
    id_vars = ['row_number'],
    value_vars=['Mod_AWDT','Obs_AWDT', 'Mod_Car','Obs_Car','Mod_MD', 'Obs_MD', 'Mod_HV', 'Obs_HV'],
    var_name='Variable',
    value_name='Value'
)
allveh_validC['Vehicle'] = allveh_validC['Variable'].str.split('_').str[-1]
allveh_validC['DataSource'] = allveh_validC['Variable'].apply(lambda x: 'Model' if 'Mod' in x else 'Observed')
allveh_validC = allveh_validC.pivot(index = ['row_number','Vehicle'],columns = 'DataSource', values = 'Value').reset_index().drop(columns={'row_number'})
allveh_validC['Diff'] = allveh_validC['Model'] - allveh_validC['Observed']
allveh_validC['Pct_Dev'] = allveh_validC['Diff'] / allveh_validC['Observed']
allveh_validC = allveh_validC.fillna(0)
allveh_validC['Vehicle'] = allveh_validC['Vehicle'].apply(lambda x: 'All' if 'AWDT' in x else x)
```


```{python}
ojs_define(validc = allveh_validC)
```


```{ojs}
viewof vehSelect = Inputs.select(new Map([['All','All'], ['Car','Car'], ['MD', 'MD'], ['HV','HV']]), {value: 'Vehicle', label: "Select Vehicle:"})
```

```{ojs}
validT = transpose(validc)
filtered_vehData = validT.filter(function(dataL) {
    return vehSelect == dataL.Vehicle;
})
```

```{ojs}
ML = require("https://www.lactame.com/lib/ml/6.0.0/ml.min.js")
vehData_lm = new ML.SimpleLinearRegression(filtered_vehData.map(d => d.Observed), filtered_vehData.map(d => d.Model))
vehData_coefficients = vehData_lm.coefficients 

vehData_corr = vehData_lm.score(filtered_vehData.map(d => d.Observed), filtered_vehData.map(d => d.Model))
```

```{ojs}

chart = Plot.plot({
  marks: [
    Plot.dot(filtered_vehData, { // Add scatterplot (blue points)
      x: "Observed",
      y: "Model",
      fill: "blue",
      r: 4,
      opacity: 0.4
    }),
    Plot.linearRegressionY(filtered_vehData, {
      x: "Observed",
      y: "Model"
    }),
    Plot.text( // Add text annotation for model estimates (shown in blue)
      [`y = ${vehData_coefficients[1].toFixed(1)}x - ${-vehData_coefficients[0].toFixed(1)}
        
        𝘙 ² = ${vehData_corr.r2.toFixed(2)}
        
        𝘳 = ${vehData_corr.r.toFixed(2)}` 
      ],
      {frameAnchor: "bottom-right", 
       dy: -20,
       fontSize: 16, 
       fill: "blue", 
       fontWeight: 600} 
    )
  ]
})
```

## Map Analysis

<mark>???Add screen shots of the TTT to get the segment-level validation map of model vs. obseved for all vehicles, cars and LT, MD, and HV???</mark>

## ATO Analysis 
Access to opportunities, also referred to as accessibility or ATO, is a way to measure how well people can connect to jobs, or vice versa. ATO metrics quantify how well the current and future transportation system work with land use.  Both shorter travel times and an increased presence of employment and other opportunities result in higher accessibility scores.  

A script to calculate ATO metrics, `1_Access_to_Opportunity.s` located in the `2_ModelScripts\7_PostProcessing` folder, has been added to the model’s `_HailMary.s` batch script and runs automatically with every model run. The script sums the number of jobs and households that are within a typical commute travel shed (in minutes) by auto and transit. The typical commute travel shed is defined using a distance decay curve estimated from the 2012 household travel survey. Metrics that combine the jobs and households are also calculated.  

Results from the ATO script are output into `7_PostProcessing\Access_to_Opportunity_@DemographicYear@.dbf`.  Results can be joined with the TAZ shapefile to visualize the data, such as is shown in @fig-ato1 and @fig-ato2. 

<mark>??? Using the CompJobHHByTran and CompJobHHByAuto fields...is that right???</mark

```{python}
import json
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt #if using matplotlib
ato_data = pd.read_csv('data/assign/Access_to_Opportunity_2019.csv')
geojson_path = r'data/assign/tazNew.geojson'
```

```{python}
geojson_data = gpd.read_file(geojson_path)
gdf = gpd.GeoDataFrame(ato_data, geometry=geojson_data.geometry)
```

```{python}
#|label: fig-ato1
#|fig-cap: Access to opportunity by auto mode.

fig, ax = plt.subplots(1, figsize=(16,10))
gdf.plot(column='CompJobHHByAuto', cmap='YlOrRd', linewidth=.1, ax=ax, edgecolor='black', legend=False)
ax.axis('off')

# Add legend at the bottom
cbar = plt.colorbar(ax.get_children()[0], ax=ax, orientation='vertical', pad=0.05, shrink=0.3)
cbar.set_label('Composite ATO Score')

plt.show()
```

```{python}
#|label: fig-ato2
#|fig-cap: Access to opportunity by transit mode.

fig, ax = plt.subplots(1, figsize=(16,10))
gdf.plot(column='CompJobHHByTran', cmap='YlOrRd', linewidth=.1, ax=ax, edgecolor='black', legend=False)
ax.axis('off')

# Add legend at the bottom
cbar = plt.colorbar(ax.get_children()[0], ax=ax, orientation='vertical', pad=0.05, shrink=0.3)
cbar.set_label('Composite ATO Score')

plt.show()
```